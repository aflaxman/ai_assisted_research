#!/usr/bin/env python3
"""
Verify Model Access for Fine-Tuning

This script demonstrates that we have full access to the model internals
needed for fine-tuning, including all parameters and gradients.
"""

from pocket_tts.models.tts_model import TTSModel
import torch


def main():
    print("ðŸ” Verifying Pocket TTS Model Access for Fine-Tuning")
    print("="*70)

    # Load the model
    print("\n1. Loading model...")
    tts = TTSModel.load_model()
    print(f"   âœ“ Model loaded: {tts.__class__.__name__}")

    # Access internal components
    print("\n2. Accessing internal components...")
    print(f"   âœ“ Flow LM: {tts.flow_lm.__class__.__name__}")
    print(f"   âœ“ Mimi (VAE): {tts.mimi.__class__.__name__}")
    print(f"   âœ“ Text Conditioner: {tts.flow_lm.conditioner.__class__.__name__}")
    print(f"   âœ“ Transformer: {tts.flow_lm.transformer.__class__.__name__}")
    print(f"   âœ“ Flow Network (MLP): {tts.flow_lm.flow_net.__class__.__name__}")

    # Count parameters
    print("\n3. Analyzing trainable parameters...")

    total_params = sum(p.numel() for p in tts.flow_lm.parameters())
    trainable_params = sum(p.numel() for p in tts.flow_lm.parameters() if p.requires_grad)

    print(f"   Total parameters: {total_params:,}")
    print(f"   Trainable parameters: {trainable_params:,}")
    print(f"   Percentage trainable: {100 * trainable_params / total_params:.1f}%")

    # Show parameter breakdown
    print("\n4. Parameter breakdown by component:")

    components = {
        "Text Conditioner": tts.flow_lm.conditioner,
        "Transformer": tts.flow_lm.transformer,
        "Flow Network (MLP)": tts.flow_lm.flow_net,
        "Other (norms, etc.)": None  # Will compute as remainder
    }

    component_params = {}
    for name, component in components.items():
        if component is not None:
            n_params = sum(p.numel() for p in component.parameters())
            component_params[name] = n_params
            trainable = sum(p.numel() for p in component.parameters() if p.requires_grad)
            print(f"   â€¢ {name}: {n_params:,} ({trainable:,} trainable)")

    # Show some parameter names
    print("\n5. Sample parameter names (first 10):")
    for i, (name, param) in enumerate(tts.flow_lm.named_parameters()):
        if i >= 10:
            print(f"   ... ({total_params // 1000}k more parameters)")
            break
        grad_status = "âœ“" if param.requires_grad else "âœ—"
        print(f"   {grad_status} {name}: {tuple(param.shape)}")

    # Test forward pass
    print("\n6. Testing forward pass...")
    try:
        # Create dummy input
        text = "Hello world"
        voice_state = tts.get_state_for_audio_prompt("javert")

        with torch.no_grad():
            # This demonstrates we can call the model
            text_emb = tts._prepare_text(text)[0]
            print(f"   âœ“ Text embedding shape: {text_emb.shape}")
            print(f"   âœ“ Forward pass works!")

    except Exception as e:
        print(f"   âœ— Error: {e}")

    # Test gradient computation
    print("\n7. Testing gradient computation...")
    try:
        # Enable gradients for a small test
        dummy_input = torch.randn(1, 10, tts.flow_lm.ldim, requires_grad=False)
        dummy_cond = torch.randn(1, 20, tts.flow_lm.dim, requires_grad=False)

        # Create a simple forward pass through the transformer
        embedded = tts.flow_lm.input_linear(dummy_input)

        # Initialize model state
        model_state = {
            'offset': torch.zeros(1, dtype=torch.long),
            'state': [None] * len(tts.flow_lm.transformer.layers)
        }

        # Forward pass
        output, _ = tts.flow_lm.transformer(embedded, model_state)

        # Compute a dummy loss
        target = torch.randn_like(output)
        loss = torch.nn.functional.mse_loss(output, target)

        print(f"   âœ“ Dummy loss computed: {loss.item():.4f}")

        # Backprop
        loss.backward()
        print(f"   âœ“ Gradient computation works!")

        # Check if gradients were computed
        has_grad = sum(1 for p in tts.flow_lm.parameters() if p.grad is not None)
        print(f"   âœ“ {has_grad} parameters have gradients")

    except Exception as e:
        print(f"   âœ— Error: {e}")
        import traceback
        traceback.print_exc()

    # Summary
    print("\n" + "="*70)
    print("ðŸ“Š Summary:")
    print("="*70)
    print(f"âœ… Model architecture: Fully accessible")
    print(f"âœ… Model weights: {total_params:,} parameters")
    print(f"âœ… Gradient computation: Working")
    print(f"âœ… Forward pass: Working")
    print(f"âœ… Fine-tuning: POSSIBLE!")
    print("\nðŸ’¡ You have everything needed to implement fine-tuning!")
    print("   See FINE_TUNING_ANALYSIS.md for detailed instructions.")
    print("="*70)


if __name__ == "__main__":
    main()
